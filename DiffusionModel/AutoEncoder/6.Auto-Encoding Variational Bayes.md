**ë…¼ë¬¸ìš”ì•½**

ë³€ë¶„ ë² ì´ì¦ˆ(Variational Bayes) ë°©ë²•ì„ ì˜¤í† ì¸ì½”ë”ì™€ ê²°í•©í•˜ì—¬ ìƒì„±ëª¨ë¸ì„ ê°œì„ í•˜ëŠ” ê¸°ë²•ì„ ì œì•ˆí•˜ëŠ” ë‚´ìš©ì„ ë‹¤ë£¹ë‹ˆë‹¤.
ì´ ë…¼ë¬¸ì€ ì£¼ë¡œ ìƒì„± ëª¨ë¸ë§ê³¼ í™•ë¥ ì  ì¶”ë¡  ë¶„ì•¼ì—ì„œ í™œìš©ë˜ë©°, ë”¥ëŸ¬ë‹ ê¸°ë°˜ì˜ í™•ë¥ ì ì¸ ë°©ë²•ë¡ ì„ ì†Œê°œí•˜ëŠ” ì¤‘ìš”í•œ ì‘í’ˆì…ë‹ˆë‹¤.

**1.** ì˜¤í† ì¸ì½”ë”ë¥¼ í™œìš©í•œ ìƒì„± ëª¨ë¸ë§ê³¼ í™•ë¥ ì  ì¶”ë¡  ë°©ë²•ì˜ ê²°í•©ì„ ì œì•ˆ : ì´ë¥¼ "Variational Autoencoder" (VAE)ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.
**2.** VAEëŠ” ë°ì´í„°ì˜ ìƒì„± ë° ì¸ì½”ë”© ê³¼ì •ì„ í™•ë¥  ë¶„í¬ë¥¼ í†µí•´ ëª¨ë¸ë§í•˜ê³ , ìƒì„±ëœ ë°ì´í„°ê°€ ì£¼ì–´ì§„ ìƒí™©ì—ì„œ ì ì¬ ë³€ìˆ˜ë¥¼ ì¶”ë¡ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë™ì‘
**3.** VAEì˜ ì£¼ìš” ì•„ì´ë””ì–´ëŠ” ì ì¬ ë³€ìˆ˜ì˜ ì‚¬ì „ ë¶„í¬ì™€ ì ì¬ ë³€ìˆ˜ì˜ í›„í¬ ë¶„í¬ë¥¼ ì •ê·œ ë¶„í¬ë¡œ ê°€ì •í•˜ê³ , ë³€ë¶„ ë² ì´ì¦ˆì˜ ê°œë…ì„ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê²ƒ
**4.** ë³€ë¶„ ë² ì´ì¦ˆë¥¼ í†µí•´ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ìƒ˜í”Œë§ì„ í†µí•´ ì ì¬ ë³€ìˆ˜ë¥¼ ìƒì„±í•˜ë©°, ìƒì„±ëœ ë°ì´í„°ì™€ ì›ë³¸ ë°ì´í„° ê°„ì˜ ì°¨ì´ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµë©ë‹ˆë‹¤.

ì´ ë…¼ë¬¸ì€ ì˜¤í† ì¸ì½”ë”ì™€ í™•ë¥ ì  ì¶”ë¡ ì˜ ê²°í•©ì„ í†µí•´ ìƒì„± ëª¨ë¸ì„ ê°œì„ í•˜ê³  ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ì—ì„œ í™œìš©ë˜ëŠ” ì¤‘ìš”í•œ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí•œ ê²ƒìœ¼ë¡œ í‰ê°€ë°›ê³  ìˆìŠµë‹ˆë‹¤.
VAEëŠ” ì´ë¯¸ì§€ ìƒì„±, ë°ì´í„° ì„ë² ë”©, ìƒì„±ì  ëª¨ë¸ë§, íŠ¹ì§• í•™ìŠµ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë˜ë©°, ë” ì¼ë°˜ì ìœ¼ë¡œëŠ” í™•ë¥ ì  ë°ì´í„° ëª¨ë¸ë§ê³¼ ë”¥ëŸ¬ë‹ì„ ê²°í•©í•œ ê¸°ë²•ì˜ ì„ êµ¬ìì ì¸ ë…¼ë¬¸ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.

ì°¸ê³  ë…¼ë¬¸ ë¦¬ë·° : https://taeu.github.io/paper/deeplearning-paper-vae/

[1] VAEëŠ” Generative Model ì´ë‹¤.
- Generative Model ì´ë€ Traning dataê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì´ training dataê°€ ê°€ì§€ëŠ” realë¶„í¬ì™€ ê°™ì€ ë¶„í¬ì—ì„œ samplingëœ ê°’ìœ¼ë¡œ new dataë¥¼ ìƒì„±í•˜ëŠ” modelì„ ë§í•œë‹¤.
  1. í›ˆë ¨ë°ì´í„°ë¶„ì„ : Generative Modelì€ ë¨¼ì € ì£¼ì–´ì§„ í›ˆë ¨ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì´ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
  2. ìƒˆë¡œìš´ ë°ì´í„° ìƒì„± : í•™ìŠµëœ ë¶„í¬ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì€ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ìƒˆë¡œìš´ ë°ì´í„°ëŠ” í›ˆë ¨ ë°ì´í„°ì™€ ìœ ì‚¬í•œ íŠ¹ì„±ì„ ê°€ì§€ë©°, ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì€ ëª¨ë¸ì¼ í•™ìŠµí•œ ë°ì´í„°ì˜ í†µê³„ì  íŠ¹ì„±ì„ ë”°ë¼ê°€ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•¨
  3. í™•ë¥  ë¶„í¬ ëª¨ë¸ë§ : Geterative Modelì€ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ í™•ë¥ ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë°ì´í„°ì˜ ë³€ë™ì„±ì´ë‚˜ ë¶ˆí™•ì‹¤ì„±ì„ ê³ ë ¤í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
 
- ìš”ì•½ : VAE(Variational Autoencoder) ì—­ì‹œ Generative Modelì˜ í•œ ì¢…ë¥˜ë¡œì„œ, ì£¼ì–´ì§„ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ëª¨ë¸ë§í•˜ê³ , í›ˆë ¨ ë°ì´í„°ì—ì„œ í•™ìŠµí•œ ë¶„í¬ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì ì¬ ë³€ìˆ˜ë¥¼ ìƒì„±í•˜ê³ , ì´ë¥¼ í†µí•´ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê¸°ëŠ¥ì„ ê°€ì§‘ë‹ˆë‹¤.
  VAEì˜ ì¥ì  ì¤‘ í•˜ë‚˜ëŠ” ë°ì´í„° ìƒì„± ê³¼ì •ì—ì„œ ì ì¬ ë³€ìˆ˜ë¥¼ í†µí•´ ë°ì´í„°ì˜ ë‹¤ì–‘ì„±ê³¼ ë¶ˆí™•ì‹¤ì„±ì„ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤.
  ì´ë¥¼ í†µí•´ VAEëŠ” ì´ë¯¸ì§€, ìŒì„±, í…ìŠ¤íŠ¸ ë“± ë‹¤ì–‘í•œ ë°ì´í„° ìœ í˜•ì— ëŒ€í•œ ìƒì„± ëª¨ë¸ë¡œ í™œìš©ë˜ë©°, ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ì—ì„œ í™œìš©ë©ë‹ˆë‹¤.

[2] í™•ë¥  í†µê³„ ì´ë¡  (Bayseain, conditional prob, pdf etc)
- ë² ì´ì§€ì•ˆ í™•ë¥ (Bayesian probability): ì„¸ìƒì— ë°˜ë³µí•  ìˆ˜ ì—†ëŠ” í˜¹ì€ ì•Œ ìˆ˜ ì—†ëŠ” í™•ë¥ ë“¤, ì¦‰ ì¼ì–´ë‚˜ì§€ ì•Šì€ ì¼ì— ëŒ€í•œ í™•ë¥ ì„ ì‚¬ê±´ê³¼ ê´€ë ¨ì´ ìˆëŠ” ì—¬ëŸ¬ í™•ë¥ ë“¤ì„ ì´ìš©í•´ ìš°ë¦¬ê°€ ì•Œê³ ì‹¶ì€ ì‚¬ê±´ì„ ì¶”ì •í•˜ëŠ” ê²ƒì´ ë² ì´ì§€ì•ˆ í™•ë¥ ì´ë‹¤.
  1. ì´ë¯¸ ì•Œë ¤ì§„ í™•ë¥ ì„ ë°”íƒ•ìœ¼ë¡œ ì•Œê³  ì‹¶ì€ ì‚¬ê±´ì˜ í™•ë¥ ì„ ì¶”ì •í•˜ëŠ” ë°©ë²•, ì•„ì§ ë°œìƒí•˜ì§€ ì•Šì€ ì‚¬ê±´ì— ëŒ€í•œ í™•ë¥ ì„ ì¶”ë¡ í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.
 
[3] ê´€ë ¨ ìš©ì–´
- latent : 'ì ì¬í•˜ëŠ”', 'ìˆ¨ì–´ìˆëŠ”', 'hidden'ì˜ ëœ»ì„ ê°€ì§„ ë‹¨ì–´. ì—¬ê¸°ì„œ ë§í•˜ëŠ” latent variable zëŠ” íŠ¹ì§•(feature)ì„ ê°€ì§„ vectorë¡œ ì´í•´í•˜ë©´ ì¢‹ë‹¤.
- intractable : ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ã…‡ìœ„í•´ í•„ìš”í•œ ì‹œê°„ì´ ë¬¸ì œì˜ í¬ê¸°ì— ë”°ë¼ ì§€ìˆ˜ì (exponential) ì¦ê°€í•œë‹¤ë©´ ê·¸ ë¬¸ì œëŠ” ë‚œí•´(intractable)í•˜ë‹¤ê³  í•œë‹¤.
- explicit density model : ìƒ˜í”Œë§ ëª¨ë¸ì˜ êµ¬ì¡°(ë¶„í¬)ë¥¼ ëª…í™•íˆ ì •ì˜
- implicit density model : ìƒ˜í”Œë§ ëª¨ë¸ì˜ êµ¬ì¡°(ë¶„í´)ë¥¼ explicití•˜ê²Œ ì •ì˜í•˜ì§€ ì•ŠìŒ
- density estimation : xë¼ëŠ” ë°ì´í„°ë§Œ ê´€ì°°í•  ìˆ˜ ìˆì„ ë•Œ, ê´€ì°°í•  ìˆ˜ ì—†ëŠ” xê°€ ìƒ˜í”Œëœ í™•ë¥ ë°€ë„í•¨ìˆ˜(probability density function)ì„ estimateí•˜ëŠ” ê²ƒ
- Gaussian distribution : ì •ê·œë¶„í¬
- Bernoulli distribution : ë² ë¥´ëˆ„ì´ë¶„í¬
- Marginal Probability : ì£¼ë³€ í™•ë¥  ë¶„í¬
- D_kl : ì¿¨ë°±-ë¼ì´ë¸”ëŸ¬ ë°œì‚°, ë‘ í™•ë¥ ë¶„í¬ì˜ ì°¨ì´
- Encode/Decode : ì•”í˜¸í™”, ë¶€í˜¸í™” / ì•”í˜¸í™”í•´ì œ, ë¶€í˜¸í™”í•´ì œ
- likelihood : ê°€ëŠ¥ë„

[4] (ë²ˆì™¸) Auto-Encoder
- VAEì™€ ì˜¤í† ì¸ì½”ë”ëŠ” ëª©ì ì´ ì „í˜€ ë‹¤ë¥´ë‹¤
- ì˜¤í† ì¸ì½”ë”ì˜ ëª©ì ì€ ì–´ë–¤ ë°ì´í„°ë¥¼ ì˜ ì••ì¶•í•˜ëŠ” ê²ƒ, ì–´ë–¤ ë°ì´í„°ì˜ íŠ¹ì§•ì„ ì˜ ë½‘ëŠ” ê²ƒ, ì–´ë–¤ ë°ì´í„°ì˜ ì°¨ì›ì„ ì˜ ì¤„ì´ëŠ” ê²ƒì´ë‹¤.
- ë°˜ë©´ VAEì˜ ëª©ì ì€ Generative modelë¡œ ì–´ë–¤ ìƒˆë¡œìš´ Xë¥¼ ë§Œë“¤ì–´ ë‚´ëŠ” ê²ƒì´ë‹¤.

### VAE
ê¸°ì¡´ì˜ ë…¼ë¬¸ì˜ íë¦„ì€ Generative Modelì´ ê°€ì§€ëŠ” ë¬¸ì œì ë“¤ì„ í•´ì†Œí•˜ê¸° ìœ„í•´ ì–´ë–¤ ë°©ì‹ì„ ë„ì…í–ˆëŠ”ì§€ ì°¨ë¡€ì°¨ë¡€ ì„¤ëª…í•˜ê³  ìˆë‹¤.
í•˜ì§€ë§Œ ê´€ë ¨ëœ ìˆ˜ì‹ë„ ë§ê³  ì¤‘ê°„ì— ìƒëµëœ ì‹ë„ ë§ì•„ ë…¼ë¬¸ëŒ€ë¡œ ë”°ë¼ê°€ë‹¤ë³´ë©´ ì „ì²´ì ì¸ êµ¬ì¡°ë¥¼ ì´í•´í•˜ê¸° í˜ë“¤ê¸° ë•Œë¬¸ì— ë¨¼ì € êµ¬ì¡°ë¥¼ ì‚´í´ë³¸ ë’¤ ê° êµ¬ì¡°ê°€ ê°€ì§€ëŠ” ì˜ë¯¸ê°€ ë¬´ì—‡ì¸ì§€ ì‚´í´ë³´ê³  ìµœì¢…ì ìœ¼ë¡œ ì •ë¦¬í•˜ë„ë¡í•œë‹¤.

### VAE GOAL

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/1e939b88-ae9c-4307-8c53-9c02f0f41288)

VAEì˜ ëª©í‘œëŠ” Generative Modelì˜ ëª©í‘œì™€ ê°™ë‹¤.
  1. dataì™€ ê°™ì€ ë¶„í¬ë¥¼ ê°€ì§€ëŠ” sample ë¶„í¬ì—ì„œ sampleì„ ë½‘ì€ ì´í›„
  2. ì–´ë–¤ ìƒˆë¡œìš´ ê²ƒì„ ìƒì„±í•´ë‚´ëŠ” ê²ƒì´ ëª©í‘œ
1. ì£¼ì–´ì§„ training dataê°€ p_data(x)(í™•ë¥ ë°€ë„í•¨ìˆ˜)ê°€ ì–´ë–¤ ë¶„í¬ë¥¼ ê°€ì§€ê³  ìˆë‹¤ë©´, sampleëª¨ë¸ p_model(x) ì—­ì‹œ ê°™ì€ ë¶„í¬ë¥¼ ê°€ì§€ë©´ì„œ (sampling ë¶€ë¶„)
2. ê·¸ ëª¨ë¸ì„ í†µí•´ ë‚˜ì˜¨ inference ê°’ì´ ìƒˆë¡œìš´ xë¼ëŠ” ë°ì´í„°ì´ê¸¸ ë°”ë€ë‹¤.(Generate ë¶€ë¶„)
  - ex) ëª‡ ê°œì˜ ë‹¤ì´ì•„ëª¬ë“œ(training data)ë¥¼ ê°€ì§€ê³  ìˆë‹¤ê³  ìƒê°í•´ ë´¤ì„ ë•Œ training ë‹¤ì´ì•„ëª¬ë“œ ë¿ë§Œ ì•„ë‹ˆë¼ ëª¨ë“  ë‹¤ì´ì•„ëª¬ë“œì˜ í™•ë¥ ë¶„í¬ì™€ ë˜‘ê°™ì€ ë¶„í¬ë¥¼ ê°€ì§„ ëª¨ë¸ì—ì„œ ê°’ì„ ë½‘ì•„ (1.smapling) trainingì„ ì‹œì¼°ë˜ ë‹¤ì´ì•„ëª¬ë“œì™€ëŠ” ë‹¤ë¥¸ ë˜ ë‹¤ë¥¸ ë‹¤ì´ì•„ëª¬ë“œë¥¼ ë§Œë“œëŠ”ê²ƒ

### VAE êµ¬ì¡°

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/73cbe276-8463-419b-b9ab-bab0c2178846)

VAE êµ¬ì¡°ë¥¼ ì™„ë²½íˆ ì •ë¦¬í•œ ê·¸ë¦¼
### 1. Encoder
- input: x â€“> ğ‘_âˆ… (ğ‘¥)â€“> ğœ‡_ğ‘–,ğœ_ğ‘–

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/c2da046d-672f-4edb-9ba7-e1ed9f1430ca)

```
img_shape = (28,28,1)
batch_size = 16
latent_dim = 2

input_img = keras.Input(shape = img_shape)
x = layers.Conv2D(32,3,padding='same',activation='relu')(input_img)
x = layers.Conv2D(64,3,padding='same',activation='relu',strides=(2,2))(x)
x = layers.Conv2D(64,3,padding='same',activation='relu')(x)
x = layers.Conv2D(64,3,padding='same',activation='relu')(x)

shape_before_flattening = K.int_shape(x) # return tuple of integers of shape of x

x = layers.Flatten()(x)
x = layers.Dense(32,activation='relu')(x)

z_mean = layers.Dense(latent_dim)(x)
z_log_var = layers.Dense(latent_dim)(x)
```
- Input shape(x) : (28, 28, 1)
- ğ‘_âˆ… (ğ‘¥) ëŠ” encoder í•¨ìˆ˜ì¸ë°, xê°€ ì£¼ì–´ì¡Œì„ë•Œ(given) zê°’ì˜ ë¶„í¬ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ ì•„ì›ƒí’‹ìœ¼ë¡œ ë‚´ëŠ” í•¨ìˆ˜ì´ë‹¤.
- ë‹¤ì‹œë§í•´ q í•¨ìˆ˜(=Encoder)ì˜ outputì€ ğœ‡_ğ‘–,ğœ_ğ‘– ì´ë‹¤.
ì–´ë–¤ Xë¼ëŠ” ì…ë ¥ì„ ë„£ì€ ì¸ì½”ë”ì˜ ì•„ì›ƒí’‹ì€ ğœ‡_ğ‘–,ğœ_ğ‘– ì´ë‹¤. ì–´ë–¤ ë°ì´í„°ì˜ íŠ¹ì§•ì„(latent variable) Xë¥¼ í†µí•´ ì¶”ì¸¡í•œë‹¤.
ê¸°ë³¸ì ìœ¼ë¡œ ì—¬ê¸°ì„œ ë‚˜ì˜¨ íŠ¹ì§•ë“¤ì˜ ë¶„í¬ëŠ” ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•œë‹¤.
ì´ëŸ° íŠ¹ì§•ë“¤ì´ ê°€ì§€ëŠ” í™•ë¥  ë¶„í¬  ğ‘_âˆ… (ğ‘¥) (ì •í™•íˆ ë§í•˜ë©´ $ì˜ true ë¶„í¬ (= $)ë¥¼ ì •ê·œë¶„í¬(=Gaussian)ë¼ ê°€ì •í•œë‹¤ëŠ” ë§ì´ë‹¤.
ë”°ë¼ì„œ latent spaceì˜ latent variable ê°’ë“¤ì€ ğ‘_âˆ… (ğ‘¥)ì˜ true ë¶„í¬ë¥¼ approximateí•˜ëŠ” ğœ‡_ğ‘–,ğœ_ğ‘–ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.

- ìš”ì•½ : ë³€ë¶„ ì˜¤í† ì¸ì½”ë”ì˜ ì¸ì½”ë” ë¶€ë¶„ì„ ì •ì˜í•œ ì½”ë“œ, VAEëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì…ë ¥ë°ì´í„°ë¥¼ ì ì¬ê³µê°„(latent space)ì— í‘œí˜„í•˜ê³ , ì´ ì ì¬ê³µê°„ì—ì„œ ë‹¤ì‹œ ì›ë˜ì˜ ì…ë ¥ ë°ì´í„°ë¥¼ ë³µêµ¬í•˜ë ¤ê³  í•˜ëŠ”êµ¬ì¡°
  1. ì ì¬ë³€ìˆ˜(Latent Variable) : VAEì—ì„œ, ì ì¬ ë³€ìˆ˜ëŠ” ë°ì´í„°ì˜ ë‚´ì¬ì ì¸ íŠ¹ì§•ì„ í‘œí˜„í•˜ëŠ” ë³€ìˆ˜ ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì–¼êµ´ ì‚¬ì§„ ë°ì´í„°ë¥¼ ë‹¤ë£° ë•Œ, ì ì¬ ë³€ìˆ˜ëŠ” ì–¼êµ´ì˜ íŠ¹ì§•(ì˜ˆì‹œ : ëˆˆì˜ í¬ê¸°, ì½” ëª¨ì–‘ ë“±)ì„ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  2. ğ‘_âˆ… (ğ‘¥) (Encoder): ì´ í•¨ìˆ˜ëŠ” ì…ë ¥ ë°ì´í„° xë¥¼ ë°›ì•„ì„œ, í•´ë‹¹ ë°ì´í„°ê°€ ì ì¬ ê³µê°„ì—ì„œ ì–´ë–¤ ìœ„ì¹˜(ë¶„í¬)ì— ìˆì„ì§€ë¥¼ í‘œí˜„í•˜ëŠ” ë‘ ê°€ì§€ ê°’, ğœ‡_ğ‘–(í‰ê· )ì™€ ğœ_ğ‘–(ë¶„ì‚°)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
  3. ì •ê·œ ë¶„í¬ (Gaussian Distribution): ì—¬ê¸°ì„œ, ì ì¬ ê³µê°„ì˜ ê° ìœ„ì¹˜ëŠ” ì •ê·œ ë¶„í¬ë¥¼ ê°€ì •í•©ë‹ˆë‹¤. ì¦‰, ê° ë°ì´í„° í¬ì¸íŠ¸ xì— ëŒ€í•´, ê·¸ ë°ì´í„°ì˜ ì ì¬ íŠ¹ì§•ì€ í‰ê·  ğœ‡_ğ‘–ì™€ ë¶„ì‚° ğœ_ğ‘–ì˜ ì •ê·œ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
     - ê°„ë‹¨íˆ ë§í•˜ë©´, ì´ ì¸ì½”ë”ëŠ” ì…ë ¥ ì´ë¯¸ì§€(ì˜ˆ : 28 x 28 í¬ê¸°ì˜ ì´ë¯¸ì§€)ë¥¼ ë°›ì•„ ê·¸ ì„ì§€ã…£ì˜ ë‚´ì¬ì  íŠ¹ì§•ì„ ì ì¬ ê³µê°„ì—ì„œì˜ ìœ„ì¹˜(í‰ê· ê³¼ ë¶„ì‚°)ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• 
     - ê·¸ë¦¬ê³  ì´ ìœ„ì¹˜ëŠ” ì •ê·œë¶„í¬ë¥¼ ê°€ì •í•˜ë¯€ë¡œ, VAEëŠ” ì´ ë¶„í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


### 2. Reparameterization Trick(Sampling)

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/61fb74e9-e7f6-49b1-8c4a-2bcf4a3f3c47)

```
def sampling(args):
    z_mean, z_log_var = args
    epsilon = K.random_normal(shape=(K.shape(z_mean)[0],latent_dim),mean=0., stddev=1.)
    return z_mean + K.exp(z_log_var) * epsilon

z = layers.Lambda(sampling)([z_mean, z_log_var])
```
ë§Œì•½ Encoder ê²°ê³¼ì—ì„œ ë‚˜ì˜¨ ê°’ì„ í™œìš©í•´ decoding í•˜ëŠ”ë° sampling í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ì–´ë–¤ ì¼ì´ ë²Œì–´ì§ˆê¹Œ? ë‹¹ì—°íˆ ëŠ” í•œ ê°’ì„ ê°€ì§€ë¯€ë¡œ ê·¸ì— ëŒ€í•œ decoder(NN)ì—­ì‹œ í•œ ê°’ë§Œ ë±‰ëŠ”ë‹¤.
ê·¸ë ‡ê²Œ ëœë‹¤ë©´ ì–´ë–¤ í•œ variableì€ ë¬´ì¡°ê±´ ë˜‘ê°™ì€ í•œ ê°’ì˜ outputì„ ê°€ì§€ê²Œ ëœë‹¤.

í•˜ì§€ë§Œ Generative Model, VAEê°€ í•˜ê³  ì‹¶ì€ ê²ƒì€, ì–´ë–¤ dataì˜ true ë¶„í¬ê°€ ìˆìœ¼ë©´ ê·¸ ë¶„í¬ì—ì„œ í•˜ë‚˜ë¥¼ ë½‘ì•„ ê¸°ì¡´ DBì— ìˆì§€ ì•Šì€ ìƒˆë¡œìš´ dataë¥¼ ìƒì„±í•˜ê³  ì‹¶ë‹¤.
ë”°ë¼ì„œ ìš°ë¦¬ëŠ” í•„ì—°ì ìœ¼ë¡œ ê·¸ ë°ì´í„°ì˜ í™•ë¥ ë¶„í¬ì™€ ê°™ì€ ë¶„í¬ì—ì„œ í•˜ë‚˜ë¥¼ ë½‘ëŠ” samplingì„ í•´ì•¼í•œë‹¤. í•˜ì§€ë§Œ ê·¸ëƒ¥ sampling í•œë‹¤ë©´ sampling í•œ ê°’ë“¤ì„ backpropagation í•  ìˆ˜ ì—†ë‹¤.
(ì•„ë˜ì˜ ê·¸ë¦¼ì„ ë³´ë©´ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤) ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ reparmeterization trickì„ ì‚¬ìš©í•œë‹¤.

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/916b45a6-90d1-45df-a97f-e8e651ea0234)

ì •ê·œë¶„í¬ì—ì„œ z1ë¥¼ ìƒ˜í”Œë§í•˜ëŠ” ê²ƒì´ë‚˜, ì…ì‹¤ë¡ ì„ ì •ê·œë¶„í¬(ìì„¸íˆëŠ” N(0,1))ì—ì„œ ìƒ˜í”Œë§í•˜ê³  ê·¸ ê°’ì„ ë¶„ì‚°ê³¼ ê³±í•˜ê³  í‰ê· ì„ ë”í•´ z2ë¥¼ ë§Œë“¤ê±°ë‚˜ ë‘ z1,z2 ëŠ” ê°™ì€ ë¶„í¬ë¥¼ ê°€ì§€ê¸° ë•Œë¬¸ì´ë‹¤.
ê·¸ë˜ì„œ ì½”ë“œì—ì„œ epsilonì„ ë¨¼ì € ì •ê·œë¶„í¬ì—ì„œ randomí•˜ê²Œ ë½‘ê³ , ê·¸ epsilonì„ exp(z_log_var)ê³¼ ê³±í•˜ê³  z_meanì„ ë”í•œë‹¤.
ê·¸ë ‡ê²Œ í˜•ì„±ëœ ê°’ì´ zê°€ ëœë‹¤.

- ìš”ì•½ : Reparameterization Trickì€ ìƒ˜í”Œë§ ê³¼ì •ì„ ë¯¸ë¶„ ê°€ëŠ¥í•˜ê²Œ ë°”ê¿”ì£¼ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. ìƒ˜í”Œë§ì„ ê·¸ëŒ€ë¡œ í•˜ë©´ ì—­ì „íŒŒê°€ ì•ˆ ë˜ê¸° ë•Œë¬¸ì—, ìƒ˜í”Œë§ ê³¼ì •ì„ ì¡°ê¸ˆ ë°”ê¾¸ì–´ ë¯¸ë¶„ ê°€ëŠ¥í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
  - Encoderê°€ ìƒì„±í•œ 'z_mean'ê³¼ 'z_log_var'ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ë”°ë¡œ ëœë¤í•œ ë…¸ì´ì¦ˆ 'epsilon'ì„ ìƒì„±
  - ì´ epsilonì€ í‘œì¤€ ì •ê·œë¶„í¬ì—ì„œ ë½‘íŒ ê°’
  - ê·¸ ë‹¤ìŒ 'z_mean'ê³¼ 'exp(z_log_var)'ì— ì´ ë…¸ì´ì¦ˆë¥¼ ì—°ì‚°í•˜ì—¬ ìµœì¢…ì ì¸ 'z'ê°’ì„ ìƒì„±
    - ì´ë ‡ê²Œí•˜ë©´ 'z'ì˜ ìƒì„±ê³¼ì •ì— ë¯¸ë¶„ ë¶ˆê°€ëŠ¥í•œ ë¶€ë¶„ì´ ì—†ì–´ì ¸ì„œ, ì „ì²´ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ ì—­ì „íŒŒë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ì¦‰, ìƒ˜í”Œë§ì„ í†µí•´ì„œë„ ê¸°ìš¸ê¸°ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.
- ê²°ë¡  : ë§Œì•½ íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ , Encoderê°€ ìƒì„±í•œ 'z_mean'ë˜ëŠ” 'z_log_var'ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œë‹¤ë©´, ëª¨ë¸ì´ ë‹¨ìˆœíˆ ë°ì´í„°ë¥¼ ì˜ ë³µì œí•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµë  ê²ƒì„
  - ì¦‰, ìƒì„±ëª¨ë¸ì˜ ëª©ì ì— ë§ì§€ ì•Šê²Œ ë˜ê¸°ì— Reparameterization Trickì€ ì´ ë¬¸ì œë¥¼ í•´ê²°í•´ì¤ë‹ˆë‹¤.

### 3. Decoder

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/a2d00862-a491-4cbd-91c0-20ca8bf7a517)

```
## 8.25 VAE decoder network, mapping latent space points to imgaes

decoder_input = layers.Input(K.int_shape(z)[1:])
x = layers.Dense(np.prod(shape_before_flattening[1:]),activation='relu')(decoder_input)
x = layers.Reshape(shape_before_flattening[1:])(x)
x = layers.Conv2DTranspose(32,3,padding='same',activation='relu',strides=(2,2))(x)
x = layers.Conv2D(1,3,padding='same',activation='sigmoid')(x)

decoder = Model(decoder_input, x)
z_decoded = decoder(z)
```

z ê°’ì„ g í•¨ìˆ˜(decoder)ì— ë„£ê³  deconv(ì½”ë“œì—ì„œëŠ” Conv2DTranspose)ë¥¼ í•´ ì›ë˜ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆì˜ ì•„ì›ƒí’‹ z_decodedê°€ ë‚˜ì˜¤ê²Œ ëœë‹¤.
ì´ë•Œ p_data(x)ì˜ ë¶„í¬ë¥¼ Bernoulli ë¡œ ê°€ì •í–ˆìœ¼ë¯€ë¡œ(ì´ë¯¸ì§€ recognition ì—ì„œ Gaussian ìœ¼ë¡œ ê°€ì •í• ë•Œë³´ë‹¤ Bernoullië¡œ ê°€ì •í•´ì•¼ ì˜ë¯¸ìƒ ê·¸ë¦¬ê³  ê²°ê³¼ìƒ ë” ì ì ˆí–ˆê¸° ë•Œë¬¸) output ê°’ì€ 0~1 ì‚¬ì´ ê°’ì„ ê°€ì ¸ì•¼í•˜ê³ , ì´ë¥¼ ìœ„í•´ activatino functionì„ sigmoidë¡œ ì„¤ì •í•´ì£¼ì—ˆë‹¤. (Gaussian ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•˜ê³  í‘¼ë‹¤ë©´ ì•„ë˜ lossë¥¼ ë‹¤ë¥´ê²Œ ì„¤ì •í•´ì•¼í•œë‹¤.)


### 4. VAE í•™ìŠµ

Loss Fucntion ì´í•´
Loss ëŠ” í¬ê²Œ ì´ 2ê°€ì§€ ë¶€ë¶„ì´ ìˆë‹¤.

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/e412cb35-438d-414d-b4f2-41ffa447deb7)

```
 def vae_loss(self, x, z_decoded):
        x = K.flatten(x)
        z_decoded = K.flatten(z_decoded)
        xent_loss = keras.metrics.binary_crossentropy(x,z_decoded)
        kl_loss   = -5e-4*K.mean(1+z_log_var-K.square(z_mean)-K.exp(z_log_var),axis=-1)
        return K.mean(xent_loss + kl_loss)

```

Reconstruction Loss(codeì—ì„œëŠ” xent_loss)
Regularization Loss(codeì—ì„œëŠ” kl_loss)
ì¼ë‹¨ ì§ê´€ì ìœ¼ë¡œ ì´í•´ë¥¼ í•˜ìë©´,

Generative ëª¨ë¸ë‹µê²Œ ìƒˆë¡œìš´ Xë¥¼ ë§Œë“¤ì–´ì•¼í•˜ë¯€ë¡œ Xì™€ ë§Œë“¤ì–´ì§„ output, New Xì™€ì˜ ê´€ê³„ë¥¼ ì‚´í´ë´ì•¼í•˜ê³ , ì´ë¥¼ Reconstruction Loss ë¶€ë¶„ì´ë¼ê³  í•œë‹¤. ì´ë•Œ ë””ì½”ë” ë¶€ë¶„ì˜ pdfëŠ” Bernoulli ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í–ˆìœ¼ë¯€ë¡œ ê·¸ ë‘˜ê°„ì˜ cross entropyë¥¼ êµ¬í•œë‹¤

Xê°€ ì›ë˜ ê°€ì§€ëŠ” ë¶„í¬ì™€ ë™ì¼í•œ ë¶„í¬ë¥¼ ê°€ì§€ê²Œ í•™ìŠµí•˜ê²Œ í•˜ê¸°ìœ„í•´ true ë¶„í¬ë¥¼ approximate í•œ í•¨ìˆ˜ì˜ ë¶„í¬ì— ëŒ€í•œ loss termì´ Regularization Lossë‹¤.
ì´ë•Œ lossëŠ” true pdf ì™€ approximated pdfê°„ì˜ D_kl(ë‘ í™•ë¥ ë¶„í¬ì˜ ì°¨ì´(ê±°ë¦¬))ì„ ê³„ì‚°í•œë‹¤.


























