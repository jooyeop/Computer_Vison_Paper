**논문요약**

변분 베이즈(Variational Bayes) 방법을 오토인코더와 결합하여 생성모델을 개선하는 기법을 제안하는 내용을 다룹니다.
이 논문은 주로 생성 모델링과 확률적 추론 분야에서 활용되며, 딥러닝 기반의 확률적인 방법론을 소개하는 중요한 작품입니다.

**1.** 오토인코더를 활용한 생성 모델링과 확률적 추론 방법의 결합을 제안 : 이를 "Variational Autoencoder" (VAE)라고 부릅니다.
**2.** VAE는 데이터의 생성 및 인코딩 과정을 확률 분포를 통해 모델링하고, 생성된 데이터가 주어진 상황에서 잠재 변수를 추론하는 방식으로 동작
**3.** VAE의 주요 아이디어는 잠재 변수의 사전 분포와 잠재 변수의 후포 분포를 정규 분포로 가정하고, 변분 베이즈의 개념을 활용하여 모델을 학습하는 것
**4.** 변분 베이즈를 통해 모델을 학습하고 샘플링을 통해 잠재 변수를 생성하며, 생성된 데이터와 원본 데이터 간의 차이를 최소화하는 방식으로 학습됩니다.

이 논문은 오토인코더와 확률적 추론의 결합을 통해 생성 모델을 개선하고 다양한 응용 분야에서 활용되는 중요한 아이디어를 제시한 것으로 평가받고 있습니다.
VAE는 이미지 생성, 데이터 임베딩, 생성적 모델링, 특징 학습 등 다양한 분야에서 사용되며, 더 일반적으로는 확률적 데이터 모델링과 딥러닝을 결합한 기법의 선구자적인 논문 중 하나입니다.

참고 논문 리뷰 : https://taeu.github.io/paper/deeplearning-paper-vae/

[1] VAE는 Generative Model 이다.
- Generative Model 이란 Traning data가 주어졌을 때 이 training data가 가지는 real분포와 같은 분포에서 sampling된 값으로 new data를 생성하는 model을 말한다.
  1. 훈련데이터분석 : Generative Model은 먼저 주어진 훈련 데이터를 분석하고 이 데이터의 분포를 파악합니다.
  2. 새로운 데이터 생성 : 학습된 분포 정보를 활용하여 모델은 새로운 데이터를 생성할 수 있습니다. 이 새로운 데이터는 훈련 데이터와 유사한 특성을 가지며, 새로운 데이터를 생성하는 과정은 모델일 학습한 데이터의 통계적 특성을 따라가는 것을 목표로 함
  3. 확률 분포 모델링 : Geterative Model은 데이터의 분포를 확률적으로 모델링하는 방식을 사용합니다. 이를 통해 데이터의 변동성이나 불확실성을 고려하여 새로운 데이터를 생성할 수 있습니다.
 
- 요약 : VAE(Variational Autoencoder) 역시 Generative Model의 한 종류로서, 주어진 데이터의 분포를 모델링하고, 훈련 데이터에서 학습한 분포 정보를 활용하여 잠재 변수를 생성하고, 이를 통해 새로운 데이터를 생성하는 기능을 가집니다.
  VAE의 장점 중 하나는 데이터 생성 과정에서 잠재 변수를 통해 데이터의 다양성과 불확실성을 조절할 수 있다는 점입니다.
  이를 통해 VAE는 이미지, 음성, 텍스트 등 다양한 데이터 유형에 대한 생성 모델로 활용되며, 다양한 응용 분야에서 활용됩니다.

[2] 확률 통계 이론 (Bayseain, conditional prob, pdf etc)
- 베이지안 확률(Bayesian probability): 세상에 반복할 수 없는 혹은 알 수 없는 확률들, 즉 일어나지 않은 일에 대한 확률을 사건과 관련이 있는 여러 확률들을 이용해 우리가 알고싶은 사건을 추정하는 것이 베이지안 확률이다.
  1. 이미 알려진 확률을 바탕으로 알고 싶은 사건의 확률을 추정하는 방법, 아직 발생하지 않은 사건에 대한 확률을 추론하기 위해 사용됩니다.
 
[3] 관련 용어
- latent : '잠재하는', '숨어있는', 'hidden'의 뜻을 가진 단어. 여기서 말하는 latent variable z는 특징(feature)을 가진 vector로 이해하면 좋다.
- intractable : 문제를 해결하기 ㅇ위해 필요한 시간이 문제의 크기에 따라 지수적(exponential) 증가한다면 그 문제는 난해(intractable)하다고 한다.
- explicit density model : 샘플링 모델의 구조(분포)를 명확히 정의
- implicit density model : 샘플링 모델의 구조(분폴)를 explicit하게 정의하지 않음
- density estimation : x라는 데이터만 관찰할 수 있을 때, 관찰할 수 없는 x가 샘플된 확률밀도함수(probability density function)을 estimate하는 것
- Gaussian distribution : 정규분포
- Bernoulli distribution : 베르누이분포
- Marginal Probability : 주변 확률 분포
- D_kl : 쿨백-라이블러 발산, 두 확률분포의 차이
- Encode/Decode : 암호화, 부호화 / 암호화해제, 부호화해제
- likelihood : 가능도

[4] (번외) Auto-Encoder
- VAE와 오토인코더는 목적이 전혀 다르다
- 오토인코더의 목적은 어떤 데이터를 잘 압축하는 것, 어떤 데이터의 특징을 잘 뽑는 것, 어떤 데이터의 차원을 잘 줄이는 것이다.
- 반면 VAE의 목적은 Generative model로 어떤 새로운 X를 만들어 내는 것이다.

### VAE
기존의 논문의 흐름은 Generative Model이 가지는 문제점들을 해소하기 위해 어떤 방식을 도입했는지 차례차례 설명하고 있다.
하지만 관련된 수식도 많고 중간에 생략된 식도 많아 논문대로 따라가다보면 전체적인 구조를 이해하기 힘들기 때문에 먼저 구조를 살펴본 뒤 각 구조가 가지는 의미가 무엇인지 살펴보고 최종적으로 정리하도록한다.

### VAE GOAL

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/1e939b88-ae9c-4307-8c53-9c02f0f41288)

VAE의 목표는 Generative Model의 목표와 같다.
  1. data와 같은 분포를 가지는 sample 분포에서 sample을 뽑은 이후
  2. 어떤 새로운 것을 생성해내는 것이 목표
1. 주어진 training data가 p_data(x)(확률밀도함수)가 어떤 분포를 가지고 있다면, sample모델 p_model(x) 역시 같은 분포를 가지면서 (sampling 부분)
2. 그 모델을 통해 나온 inference 값이 새로운 x라는 데이터이길 바란다.(Generate 부분)
  - ex) 몇 개의 다이아몬드(training data)를 가지고 있다고 생각해 봤을 때 training 다이아몬드 뿐만 아니라 모든 다이아몬드의 확률분포와 똑같은 분포를 가진 모델에서 값을 뽑아 (1.smapling) training을 시켰던 다이아몬드와는 다른 또 다른 다이아몬드를 만드는것

### VAE 구조

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/73cbe276-8463-419b-b9ab-bab0c2178846)

VAE 구조를 완벽히 정리한 그림
### 1. Encoder
- input: x –> 𝑞_∅ (𝑥)–> 𝜇_𝑖,𝜎_𝑖

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/c2da046d-672f-4edb-9ba7-e1ed9f1430ca)

```
img_shape = (28,28,1)
batch_size = 16
latent_dim = 2

input_img = keras.Input(shape = img_shape)
x = layers.Conv2D(32,3,padding='same',activation='relu')(input_img)
x = layers.Conv2D(64,3,padding='same',activation='relu',strides=(2,2))(x)
x = layers.Conv2D(64,3,padding='same',activation='relu')(x)
x = layers.Conv2D(64,3,padding='same',activation='relu')(x)

shape_before_flattening = K.int_shape(x) # return tuple of integers of shape of x

x = layers.Flatten()(x)
x = layers.Dense(32,activation='relu')(x)

z_mean = layers.Dense(latent_dim)(x)
z_log_var = layers.Dense(latent_dim)(x)
```
- Input shape(x) : (28, 28, 1)
- 𝑞_∅ (𝑥) 는 encoder 함수인데, x가 주어졌을때(given) z값의 분포의 평균과 분산을 아웃풋으로 내는 함수이다.
- 다시말해 q 함수(=Encoder)의 output은 𝜇_𝑖,𝜎_𝑖 이다.
어떤 X라는 입력을 넣은 인코더의 아웃풋은 𝜇_𝑖,𝜎_𝑖 이다. 어떤 데이터의 특징을(latent variable) X를 통해 추측한다.
기본적으로 여기서 나온 특징들의 분포는 정규분포를 따른다고 가정한다.
이런 특징들이 가지는 확률 분포  𝑞_∅ (𝑥) (정확히 말하면 $의 true 분포 (= $)를 정규분포(=Gaussian)라 가정한다는 말이다.
따라서 latent space의 latent variable 값들은 𝑞_∅ (𝑥)의 true 분포를 approximate하는 𝜇_𝑖,𝜎_𝑖를 나타낸다.

- 요약 : 변분 오토인코더의 인코더 부분을 정의한 코드, VAE는 기본적으로 입력데이터를 잠재공간(latent space)에 표현하고, 이 잠재공간에서 다시 원래의 입력 데이터를 복구하려고 하는구조
  1. 잠재변수(Latent Variable) : VAE에서, 잠재 변수는 데이터의 내재적인 특징을 표현하는 변수 입니다. 예를 들어, 얼굴 사진 데이터를 다룰 때, 잠재 변수는 얼굴의 특징(예시 : 눈의 크기, 코 모양 등)을 표현할 수 있습니다.
  2. 𝑞_∅ (𝑥) (Encoder): 이 함수는 입력 데이터 x를 받아서, 해당 데이터가 잠재 공간에서 어떤 위치(분포)에 있을지를 표현하는 두 가지 값, 𝜇_𝑖(평균)와 𝜎_𝑖(분산)을 반환합니다.
  3. 정규 분포 (Gaussian Distribution): 여기서, 잠재 공간의 각 위치는 정규 분포를 가정합니다. 즉, 각 데이터 포인트 x에 대해, 그 데이터의 잠재 특징은 평균 𝜇_𝑖와 분산 𝜎_𝑖의 정규 분포를 따른다고 가정하는 것입니다.
     - 간단히 말하면, 이 인코더는 입력 이미지(예 : 28 x 28 크기의 이미지)를 받아 그 임지ㅣ의 내재적 특징을 잠재 공간에서의 위치(평균과 분산)로 변환하는 역할
     - 그리고 이 위치는 정규분포를 가정하므로, VAE는 이 분포를 사용하여 새로운 데이터 포인트를 생성할 수 있습니다.














