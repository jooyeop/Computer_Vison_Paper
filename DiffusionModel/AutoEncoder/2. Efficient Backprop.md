**속도향상**
신경망의 역전파 알고리즘을 개선하여 훈련속도를 크게 향상
이를 위해 미니배치 기법을 사용하여 가중치 갱신을 미니 배치 단위로 수행
이는 전체 데이터셋을 사용하는 것 보다 훨씬 빠른 수렴을 도모

**학습률스케쥴링**
학습률을 조절하는 기법
초기에는 큰 학습률을 사용하고 학습이 진행됨에 따라 점진적으로 학습률을 줄여나가는 방식을 채택하여 안정적인 훈련을 도모

**가중치 초기화**
가우시안분포를 이용하여 가중치를 초기화 하는 방법을 소개하여 초기 훈련 과정 개선

**예비처리 기법**
입력 데이터의 사전 처리(preprocessing) 기법을 적용하여 훈련 속도를 높이는 방법을 제시
이를 통해 데이터의 특성을 더 잘 캡처하고 훈련을 더 효율적으로 수행

이 논문은 역전파 알고리즘의 효율성을 향상시키는 다양한 기법을 제시하여 딥러닝 모델의 훈련 과정을 최적화하는 방법을 탐구한 첫 번째 논문 중 하나로 평가되며, 이후 딥러닝의 발전과 확산에 큰 영향을 미쳤습니다.

### 일반적인 gradient-based 학습 모델
**(Learning and Generalization)**

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/f817264e-4ab8-42a6-99a1-c45dcd1ee753)

논문에서 이야기하는 모델의 모습
입력값 : input은 Z
학습가능한 Parameter : W
모델에 W, Z가 들어가면 output M(W, Z)로 나옴

output M(W,Z)와 desired output D를 가장 일반적인 cost function인 **mean-square**를 이용하여 E를 산출
E(error)는 모델의 성능을 평가하는 유일한 스칼라 값
E(error)의 값이 작을수록 모델이 학습을 잘 했다고 말할 수 있음

즉, 머신러닝의 핵심은
**cost function 을 통해 나온 E 값을 줄이는 방법을 찾는 일**
위와 같이 Gradient-based 학습 모델은 이 E를 줄이는 **Back Propagation(역전파)** 이라는 과정을 통해 수행
그 과정에서 Gradient가 쓰이기 때문에 Gradient based 학습 모델이라고 부르는 것

### 기본적인 Back Propagation
**(Standard Back Propagation)**
효과적인 Back Propagation 방법 참고 링크
http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html

**Gradient Descent 알고리즘을 통한 Back Propagtion**
Back Propagation 은 학습 모델에서 W(trainable parameter)를 업데이틑 하는 방법
W의 값을 업데이트해서 cost function의 결과값인 error가 작아지게 하면 성공
이 W의 업데이트 과정(역전파)을 수백번 수천벅 반복해서 천천히 error를 줄여나가는 것

앞에 정리한 역전파 논문을 참고하면 됨

###(메인) Back Propagation 으로 학습하는 모델들의 학습 성능을 높이는 Trick 들
**(Few Practical Tricks)**
4-1 Stochastic vs Batch Learning

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/5ec4baf7-7f36-4dc9-992e-28b6149974b7)

**Stochastic Gradient Descent**
데이터셋이 매우 큰경우, 매번 전체 학습 데이터를 연산하는 것은 메모리 사용량이 늘어나는 일임
하나의 데이터를 골라 인공신경망에 학습시킴
**(full) Batch Gradient Descent**
전체 학습 데이터의 gradient를 평균내어 GD의 매번의 step마다 적용
full batch 와는 다르게 확률론적, 불확실성의 개념 도입

4-2 Shuffling the examples
일반적으로 모델의 의미있는 순서로 훈련 예제를 제공하는것을 피하기를 원함
이유는 최적화 알고리즘에 편향될 수 있기 때문
결과적으로 매 epoch 이후에 교육 데이터를 섞는 것이 가장 좋음

4.3 Normalizing inputs

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/e1035daf-977f-4162-9faa-efba3bc6abaa)

전체 구간을 0~1사이의 값으로 맞춰 준다.
학습을 용이하게 하기 위해 일반적으로 매개 변수의 초기 값을 제로 평균 및 단위 분산으로 초기화하여 정규화
정규화를 모델 아키텍처의 일부로 만들어줌으로써 더 높은 학습 속도를 사용하고 초기화 매개 변수에 덜주의를 기울일 수 있음
일괄 정규화는 정규 표현식으로 추가 기능을 수행하여 삭제 필요성을 줄임

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/0c6554d4-bffc-4185-a973-d406aff04330)


4.4 The Sigmoid

4.5 Choosing Target Value

4.6 Initializing Weight

4.7 Choose Learning Rate
