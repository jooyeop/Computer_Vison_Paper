### 논문 간략 정리
1. 신경망의 구조 : 뉴런들이 여러 레이어로 구성되어 있으며, 각 레이어는 다음 레이어로의 연결을 통해 정보를 전달합니다.
2. 가중치 : 신경망의 각 연결에는 가중치가 있으며, 이 가중치는 학습 과정에서 조정됩니다.
3. 활성화함수 : 각 뉴런의 출력은 활성화 함수를 통해 변환되어 다음 레이어로 전달됩니다.
4. 오류 역전파 : 네트워크의 출력 오류를 최소화하기 위해 가중치를 조정하는 매커니즘 입니다.


**속도향상**
신경망의 역전파 알고리즘을 개선하여 훈련속도를 크게 향상
이를 위해 미니배치 기법을 사용하여 가중치 갱신을 미니 배치 단위로 수행
이는 전체 데이터셋을 사용하는 것 보다 훨씬 빠른 수렴을 도모

**학습률스케쥴링**
학습률을 조절하는 기법
초기에는 큰 학습률을 사용하고 학습이 진행됨에 따라 점진적으로 학습률을 줄여나가는 방식을 채택하여 안정적인 훈련을 도모

**가중치 초기화**
가우시안분포를 이용하여 가중치를 초기화 하는 방법을 소개하여 초기 훈련 과정 개선

**예비처리 기법**
입력 데이터의 사전 처리(preprocessing) 기법을 적용하여 훈련 속도를 높이는 방법을 제시
이를 통해 데이터의 특성을 더 잘 캡처하고 훈련을 더 효율적으로 수행

이 논문은 역전파 알고리즘의 효율성을 향상시키는 다양한 기법을 제시하여 딥러닝 모델의 훈련 과정을 최적화하는 방법을 탐구한 첫 번째 논문 중 하나로 평가되며, 이후 딥러닝의 발전과 확산에 큰 영향을 미쳤습니다.

### 일반적인 gradient-based 학습 모델
**(Learning and Generalization)**

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/f817264e-4ab8-42a6-99a1-c45dcd1ee753)

논문에서 이야기하는 모델의 모습
입력값 : input은 Z
학습가능한 Parameter : W
모델에 W, Z가 들어가면 output M(W, Z)로 나옴

output M(W,Z)와 desired output D를 가장 일반적인 cost function인 **mean-square**를 이용하여 E를 산출
E(error)는 모델의 성능을 평가하는 유일한 스칼라 값
E(error)의 값이 작을수록 모델이 학습을 잘 했다고 말할 수 있음

즉, 머신러닝의 핵심은
**cost function 을 통해 나온 E 값을 줄이는 방법을 찾는 일**
위와 같이 Gradient-based 학습 모델은 이 E를 줄이는 **Back Propagation(역전파)** 이라는 과정을 통해 수행
그 과정에서 Gradient가 쓰이기 때문에 Gradient based 학습 모델이라고 부르는 것

### 기본적인 Back Propagation
**(Standard Back Propagation)**
효과적인 Back Propagation 방법 참고 링크
http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html

**Gradient Descent 알고리즘을 통한 Back Propagtion**
Back Propagation 은 학습 모델에서 W(trainable parameter)를 업데이틑 하는 방법
W의 값을 업데이트해서 cost function의 결과값인 error가 작아지게 하면 성공
이 W의 업데이트 과정(역전파)을 수백번 수천벅 반복해서 천천히 error를 줄여나가는 것

앞에 정리한 역전파 논문을 참고하면 됨

###(메인) Back Propagation 으로 학습하는 모델들의 학습 성능을 높이는 Trick 들
**(Few Practical Tricks)**
4-1 Stochastic vs Batch Learning

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/5ec4baf7-7f36-4dc9-992e-28b6149974b7)

**Stochastic Gradient Descent**
데이터셋이 매우 큰경우, 매번 전체 학습 데이터를 연산하는 것은 메모리 사용량이 늘어나는 일임
하나의 데이터를 골라 인공신경망에 학습시킴
**(full) Batch Gradient Descent**
전체 학습 데이터의 gradient를 평균내어 GD의 매번의 step마다 적용
full batch 와는 다르게 확률론적, 불확실성의 개념 도입

4-2 Shuffling the examples
일반적으로 모델의 의미있는 순서로 훈련 예제를 제공하는것을 피하기를 원함
이유는 최적화 알고리즘에 편향될 수 있기 때문
결과적으로 매 epoch 이후에 교육 데이터를 섞는 것이 가장 좋음

4.3 Normalizing inputs

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/e1035daf-977f-4162-9faa-efba3bc6abaa)

전체 구간을 0~1사이의 값으로 맞춰 준다.
학습을 용이하게 하기 위해 일반적으로 매개 변수의 초기 값을 제로 평균 및 단위 분산으로 초기화하여 정규화
정규화를 모델 아키텍처의 일부로 만들어줌으로써 더 높은 학습 속도를 사용하고 초기화 매개 변수에 덜주의를 기울일 수 있음
일괄 정규화는 정규 표현식으로 추가 기능을 수행하여 삭제 필요성을 줄임

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/0c6554d4-bffc-4185-a973-d406aff04330)


4.4 The Sigmoid

4.5 Choosing Target Value

4.6 Initializing Weight

4.7 Choose Learning Rate

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/52edc2e2-9094-451c-8e66-7bfef832028c)

위의 다이어그램은 기본적인 Feedforward Neural Network의 아키텍처를 보여줍니다.

Feedforward Neural Network:
신경망은 여러 개의 레이어로 구성되어 있으며, 각 레이어는 여러 개의 뉴런으로 구성됩니다. 각 뉴런은 입력값과 가중치를 곱한 합을 활성화 함수를 통해 변환하여 출력값을 생성합니다.

파란색 원: 입력 레이어의 뉴런
녹색 원: 은닉 레이어의 뉴런
빨간색 원: 출력 레이어의 뉴런
화살표: 뉴런 간의 연결을 나타냅니다. 각 연결에는 가중치가 있습니다.

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/a75f6739-e00b-45f0-b77c-53e0dff4b063)

Backpropagation:
오류(타겟 값과 신경망의 예측 값의 차이)를 계산한 후, 이 오류를 역방향으로 전파하여 각 뉴런의 가중치를 업데이트합니다. 이 과정은 연쇄 법칙을 사용하여 각 뉴런에 대한 오류의 편미분을 계산하는 것을 포함합니다.

보라색 화살표: 오류를 역전파하여 이전 레이어로 전달하는 것을 나타냅니다.

Backpropagation의 주요 아이디어는 네트워크의 출력에서 발생하는 오류를 역방향으로 전파하고, 이 오류를 사용하여 각 연결의 가중치를 조정하는 것입니다. 이 과정은 연쇄 법칙을 사용하여 각 뉴런에 대한 오류의 편미분을 계산합니다.

이러한 방식으로 신경망은 예상 출력과 실제 출력 간의 차이를 최소화하려고 학습합니다. 각 연결의 가중치는 오류를 줄이기 위해 반복적으로 조정됩니다. 이 과정은 데이터셋의 모든 샘플에 대해 여러 번 반복됩니다.

이 논문은 이러한 방식으로 신경망이 복잡한 함수와 패턴을 학습할 수 있음을 보여주었으며, 이로 인해 딥러닝 연구의 주요 발전이 이루어졌습니다.


