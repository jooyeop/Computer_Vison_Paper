Learning representations by back-propagating errors 논문 요약
Back-Propagation[역전파]
해당 학습 과정에서 뉴런들 사이의 연결 가중치를 조절해서, 신경망의 결과와 실제로 사용자가 기대했던 결과 사이의 오차를 최소화 하도록 한다.
-> input/output layer가 아닌 hidden layer들도 의미 있는 특성을 학습할 수 있으며 원하는 결과를 제공할 수 있음
Symmetric[대칭성]한 구졸르 학습하려면 Hidden node 층을 사용해야 한다. [ 아래 그림 ]

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/a604b05f-27bb-41a1-bc0a-da15652a2901)

Layered network에서 input layer는 가장 낮은층에 위치하며, output layer는 가장 높은 층에 위치함
이전 i 번째 뉴런의 출력을 yi라고 하고, i번째 뉴런에서 j 번째 뉴런으로의 연결 가중치는 wji라고 할 때, j 번 째 뉴런에 대한 가중 입력 xj 는 다음과 같이 정의한다.

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/b23f357d-1381-4e17-90f0-58615d34f50b)

j번 째 뉴런이 가중 입력 x_j를 받았을 때 출력 y_j는 다음과 같이 정의하ㅣㄴ다. 해당 식은 가중 입력에 대한 non-linear function

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/c6d59ccf-2bae-4c8d-9a86-ec152c3e0fed)

학습의 최종 목표 : 우리가 원하는 태스크를 수행할 수 있는(원하는 결과를 도출할 수 있는) 가중치(weight)를 찾아내야함
예측한 결과가 일치하는지 확인하기 위해 loss를 정의해야함.
전체에러(Total loss)는 다음과 같이 정의할 수 있다.

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/3b0e9c16-5174-4731-ad95-35562041b0b9)


정확한 값의 가중치를 찾아 냈다면, output과 실제 정답이 유사함 그러면 loss[손실함수]는 0에 가까움
학습을 통해 loss를 최소화 시켜야 한다.
Back-Propagation[역전파]을 통해서 가중치[weight]를 변화 시켰을때, 해당 변화가 loss에 어느 정도 영향이 있는지 확인하고 loss의 값을 최소화 시키는 방향으로 가중치를 업데이트 해야함
loss 미분식

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/a119bf74-ae96-4869-a991-6d27708b18bf)
loss 미분식을 기반으로 chain rule을 적용하면 전체 로스 E를 가중입력, x_j에 대해 미분한 값을 구할 수 있다.
x에 대해서 미분하면 y(1-y)
x가 전체 loss에 어느정도 영향을 끼치는지 알 수 있는 식

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/e1ddf34b-4015-412d-8fc5-e02254fea0cb)

동일한 방법으로 전체 loss E를 가중치[weight/w]에 대해서 미분하면 구할 수 있다.

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/cd4584ab-d4c3-4f38-bd50-7d5b5feff29a)

지금까지 적은 모든식을 반복해서 가중치를 업데이트하고, 최적의 가중치값을 찾아 가장 낮은 loss값을 찾아야 한다.
