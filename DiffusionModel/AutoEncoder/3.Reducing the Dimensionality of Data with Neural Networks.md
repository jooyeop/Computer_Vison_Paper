### 논문 요약
**1. Deep Autoencoder**
Deep Autoencoder라는 신경망 구조를 소개
이는 입력데이터를 저차원 표현으로 인코딩하고, 다시 원래 차원으로 디코딩하여 재구성하는 방법
이를 통해 데이터의 중요한 특성을 학습하면서 차원을 줄일 수 있음

**2.Layerwise Pretraining**
Deep Autoencoder의 학습을 개선하기 위해 계층별 사전 훈련(Layerwise Pretraining) 기법을 도입
이는 신경망의 각 층을 개별적으로 사전 훈련하고, 이를 조합하여 전체 신경망을 학습하는 방식
이를 통해 초기 가중치를 더 잘 설정하고, 학습이 빠르게 수렴되도록 도움을 줌

**3.비지도학습**
이 논문은 주로 비지도 학습 기법을 사용하여 데이터의 특성을 학습하는데 초점을 맞춤
이는 레이블이 없는 데이터를 사용하여 데이터의 구조와 특성을 발견하는 유용

**4.차원축소의장점**
논문은 차원 축소가 머신러닝 문제에서 주요 장점을 가지는 경우를 다룸
차원 축소를 통해 데이터의 특성을 시각화하고, 노이즈를 제거하며, 효율적인 특성 추출을 할 수 있는 장점을 강조

이 논문은 신경망을 사용한 차원 축소의 기초를 다루며, 비지도 학습과 특성 학습에 대한 기본 개념을 제시

참고한 논문 리뷰 사이트 : https://velog.io/@pabiya/Reducing-the-Dimensionality-ofData-with-Neural-Networks

작은 Central layer를 가진 multilayer neural network를 학습하여 High-dimensional data를 low dimensinal codes로 전환하고 다시 gigh-dimensional input vectors를 복원 할 수 있다.
그러나, 이는 initial weight가 좋은 해에 근접할 때만 잘 작동한다.
논문은 데이터의 차원을 축소하는 데 PCA보다 효과적으로 deep autoencoder networks가 low-dimensinal codes를 학습하는 weight initializing 방법을 소개한다.
- 작은 중앙 층을 가진 다층 신경망을 사용하여 고차원 데이터를 저차원 코드로 변환하고, 다시 고차원 입력벡터를 복원하는 개념
- 이러한 변환작업은 초기 가중치가 좋은 값에 가까울 때에만 잘 작동한다는 한계가 있음
- 이런 문제를해결하기 위해 논문은 차원 축소를 위해 PCA보다 더 효과적으로 동작하는 딥 오토인코더 네트워크가 저차원 코드를 학습하는 가중치 초기화 방법을 제안
**작은 중앙 층을 가진 다층 신경망** : 이는 여러 층으로 구성된 신경망 구조입니다. 그 중앙에 있는 층이 데이터의 저차원 표현을 학습하고, 이후의 층들이 해당 표현을 사용하여 입력 데이터를 복원하는 역할을 수행
**좋은 초기 가중치** : 딥러닝 모델을 효과적으로 학습시키기 위해서는 초기 가중치 설정이 중요합니다. 초기 가중치 설정이 잘못되면 모델의 수렴이 어려울 수 있습니다.
**PCA보다 효과적인 딥 오토인코더 네트워크** : 논문에서 소개하는 딥 오토인코더 네트워크가 주어진 데이터에 대해 더 효과적인 저차원 코드를 학습할 수 있다는 것을 의미합니다.
  PCA는 선형 변환을 사용하여 차원을 축소하는 방법이지만, 딥 오토인코더는 비선형 변환을 통해 데이터의 복잡한 특성을 캡처할 수 있습니다.
**가중치 초기화 방법** :  논문은 딥 오토인코더 네트워크의 가중치 초기화 방법을 제안합니다. 이 초기화 방법은 작은 중앙 층에서 충분한 정보를 얻을 수 있도록 가중치를 설정하며, 이를 통해 초기 학습 단계를 더 잘 수행할 수 있습니다.

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/073e370a-f449-4b95-af61-3b00011e0274)

랜덤 weights를 가진 두 network에서 시작해 original data롸 reconstruction의 차이를 최소화 하도록 두 네트워크가 학습된다.
**두 개의 네트워크** : 오토인코더는 일반적으로 인코더(encoder)와 디코더(decoder) 두 개의 신경망으로 구성됩니다.
인코더는 입력 데이터를 저차원 코드로 변환하는 역할을 하며, 디코더는 이 저차원 코드를 다시 원래의 고차원 입력 데이터로 복원하는 역할을 합니다.
**랜덤 Weights** : 학습 초기에는 인코더와 디코더의 가중치는 랜덤하게 초기화됩니다. 이는 오토인코더가 초기에는 입력과 출력 사이의 관계를 잘 파악하지 못하고 무작위로 출력을 생성할 수 있음을 의미합니다.
**Reconstruction Loss 최소화** : 오토인코더의 목표는 원래 데이터와 복원된 데이터 사이의 차이를 최소화하는 것입니다.
이를 위해 학습 중에 오토인코더는 입력 데이터를 인코더로 통과시켜 저차원 코드를 생성한 다음, 디코더로 통과시켜 복원된 데이터를 생성합니다.
이 복원된 데이터와 원래 데이터 사이의 차이를 최소화하는 방향으로 가중치를 조정합니다.
**학습 과정** : 오토인코더는 일반적인 경사 하강법과 역전파(backpropagation) 알고리즘을 사용하여 학습됩니다.
학습 과정에서 인코더와 디코더의 가중치가 조정되며, 이로 인해 원래 데이터와 복원된 데이터 간의 차이가 최소화되도록 변화합니다.
**특성 추출 및 차원 축소** : 학습이 완료된 후, 인코더의 중간 층에서 얻어진 저차원 코드는 데이터의 중요한 특성을 나타내는데 사용될 수 있습니다. 이는 차원 축소나 데이터의 특성 추출에 활용될 수 있습니다.
- 요약 : 오토인코더는 랜덤한 가중치를 가진 두 개의 신경망으로 구성 되어 있으며, 원래 데이터와 복원된 데이터 사이의 차이를 최소화 하는 방향으로 학습 된다.

2~4 층의 multiple hidden layers를 가진 nonlinear autoencoders를 최적화하는 것은 어렵다. initial weights가 크면 autoencoders는 poor local minima를 찾고 initial weights가 작으면 early layers의 gradients가 작아서 학습이 효과적이지 않다. initial weight가 good solution에 근접하면 gradient descent가 잘 작동하지만, 이를 찾는 게 쉽지 않다. 논문은 binary data에 대한 'pretraining' procedure을 소개하고 이를 real-valued data로 일반화한다.
- 비선형 오토인코더(nonlinear autoencoder)를 최적화 하는데 관련된 어려움과 이를 극복하기 위한 사전훈련(pretraining)절차에 대한 내용
**비선형 오토인코더**: 비선형 오토인코더는 입력 데이터를 저차원으로 압축하고 다시 복원하는데 사용되는 딥러닝 모델입니다. 이 모델은 중간에 여러 개의 숨겨진(hidden) 층을 갖습니다.
**Initial Weights와 Poor Local Minima**: 초기 가중치 설정이 중요한데, 가중치가 크면 오토인코더가 부적절한 지역 최소값(poor local minima)로 수렴할 가능성이 있습니다.
즉, 모델이 학습하지 못하고 더 낮은 손실값을 찾지 못하는 상황입니다.
**Early Layers의 Gradients와 작은 가중치**: 초기 가중치가 작으면 초기 층의 기울기(gradients)가 작아져 학습이 어려워집니다. 이로 인해 신경망이 깊어질수록 학습이 더욱 어려워질 수 있습니다.
**Good Solution에 근접한 Initial Weights**: 가장 이상적인 상황은 초기 가중치가 좋은 솔루션에 가까울 때입니다. 이럴 경우 경사 하강법(gradient descent)이 더 잘 작동하여 모델이 잘 수렴하게 됩니다.
**사전 훈련(Pretraining) 절차**: 이 어려움을 극복하기 위해 논문에서는 사전 훈련 절차를 도입합니다. 이는 먼저 각 층을 따로 훈련하여 초기 가중치를 설정하고, 이후에 전체 모델을 학습하는 방식입니다.
사전 훈련 단계에서는 바이너리(binary) 데이터를 사용하여 각 층의 특성을 더 쉽게 학습하도록 도와줍니다.
**실수값 데이터로의 일반화**: 사전 훈련은 바이너리 데이터에서 시작하지만, 이를 실수값(real-valued) 데이터로 일반화하여 최종 오토인코더를 학습시킵니다. 사전 훈련으로 얻은 초기 가중치는 오토인코더 학습에 좋은 시작점을 제공하며, 더 빠르고 안정적인 수렴을 도와줍니다.

ensemble of binary vectors (e.g., images)는 restricted Boltzmann machine (RBM) (5, 6)라고 불리는 two-layer network를 통해 model될 수 있다. 거기서 stochastic, binary pixels는 symmetrically weighted connections를 사용해 stochastic, binary feature detectors에 연결된다. pixels는 (그들의 states가 관측되기 때문에) RBM의 "visible" units에 상응하고 feature detectors는 "hidden" units에 상응한다. visible units와 hidden units의 joint configuration (v, h)는 다음과 같은 energy를 가진다.
- Restricted Boltzmann Machine (RBM)을 사용하여 이진 벡터(예: 이미지)의 앙상블을 모델링할 수 있다는 내용을 다루고 있습니다.
**Ensemble of Binary Vectors**: "앙상블"은 여러 개의 개별적인 요소가 모여 하나의 복합 요소를 형성하는 것을 의미합니다.
여기서는 이진 벡터들의 집합을 의미합니다. 예를 들어, 여러 개의 이진 이미지가 있는 경우, 이들을 모아서 하나의 앙상블로 간주할 수 있습니다.
**Restricted Boltzmann Machine (RBM)**: RBM은 확률적인 생성 모델 중 하나로, 각각의 뉴런이 이진 값을 가지는 두 개의 층으로 구성됩니다.
이 두 개의 층은 visible layer와 hidden layer로 구분되며, 서로 뉴런들이 연결되어 있습니다.
**Stochastic, Binary Pixels**: 이진 값(0 또는 1)을 가지며 확률적으로 결정되는 픽셀들을 나타냅니다.
예를 들어, 흑백 이미지에서 각 픽셀은 0 또는 1의 값을 가지며, 이 값은 이미지 내용에 따라 확률적으로 결정됩니다.
**Symmetrically Weighted Connections**: RBM의 연결 가중치는 대칭적으로 설정됩니다. 즉, 한 층의 뉴런과 다른 층의 뉴런 간의 연결 가중치는 양방향으로 동일하게 설정됩니다.
**Stochastic, Binary Feature Detectors**: 특징 탐지기(feature detector)는 특정한 특징을 감지하기 위한 뉴런으로, 이진 값(0 또는 1)을 가집니다. 확률적인 방식으로 활성화되며, 입력 데이터의 특정한 패턴을 찾아냅니다.
**Visible Units와 Hidden Units**: RBM의 구성 요소로서, visible units는 입력 데이터(예: 이미지 픽셀)에 상응하며, hidden units는 특징 탐지기에 상응합니다.
**Joint Configuration과 Energy**: visible units와 hidden units의 조합(연결된 상태)을 가지는 RBM의 에너지를 설명합니다. 에너지는 연결 가중치와 각 뉴런의 활성화 상태에 따라 결정됩니다.
- RBM을 사용하여 이진 벡터들의 앙상블을 모델링할 수 있는 방법과 그 과정에 대한 간략한 설명을 제시하고 있습니다. RBM은 데이터의 확률적인 특징을 모델링하고, 가중치를 통해 이진 값들 간의 관계를 학습하여 복잡한 데이터 패턴을 파악할 수 있는 강력한 도구 중 하나입니다.

![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/abf19bcf-bf4f-4915-9949-ab794b8ec5b6)


