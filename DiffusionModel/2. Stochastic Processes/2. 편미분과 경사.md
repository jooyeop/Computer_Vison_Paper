## 편미분(Partial Derivative)
  - 다변수 함수f(x, y, z, ...)가 있을 때, 특정 변수에 대한 미분을 편미분 이라고 함
  - 예를 들어, f(x, y) = x^2 + y^2 라는 함수가 있을 때, x에 대한 편미분과 y에 대한 편미분은 다음과 같이 표현 됨

    ![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/b77a23a0-5f1b-4729-84ea-87c312b5c61d)



## 경사(Gradient)
  - 경사는 다변수 함수에서 각 변수에 대한 편미분을 벡터 형태로 나타낸 것
  - 경사는 함수의 '최대 증가 방향'을 가르킴
    
  ![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/1b237745-3df2-4502-868d-7761c109e9db)

  - 예를 들어, f(x, y) = x^2 + y^2의 경사는 ∇f(x, y) = (2x, 2y)


## 경사 하강법(Gradient Descent)
  - 경사 하강법은 최적화 문제, 특히 비용 함수 J(θ)를 최소화하는 θ를 찾을 때 사용되는 알고리즘 입니다.
  - 각 단계에서 현재의 θ값에서 경사 ∇J(θ)를 계산하고, 경사의 반대방향으로 θ를 업데이트 합니다.
    
    ![image](https://github.com/jooyeop/Computer_Vison_Paper/assets/97720878/5150b475-9423-4dc2-9b58-7253ffe28a02)

    - 여기서 α는 학습률(learning rate)이라고 하며, 얼마나 빠르게 최소값을 찾을 것인지를 결정 합니다.
    - θ는 일반적으로 모델의 파라미터를 의미
    - 딥러닝에서는 이 θ가 신경망의 가중치와 편향을 포함할 수 있음
    - 경사하강법은 이 값을 업데이트하여 손실 함수를 최소화함
