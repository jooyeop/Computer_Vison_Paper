import tensorflow as tf
from tensorflow.keras import layers, models

#입력 이미지 크기는 28*28*1 (흑백 이미지)로 가정
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))

'''
1. 필터의 갯수
필터의 갯수는 합성곱 계층에서 학습하려는 특징들의 수를 나타냄,
예를들면 첫번 째 합성곱 계층에서는 이미지의 기본적인 특징들을 학습함, 32개인 이유는 초기 계층에서는 그림의 기본적인 특징을 추출하기 위해 여러 필터를 사용하지만,
32는 흔히 사용되는 값 중 하나이며, 다른 값으로도 사용 가능함 예를들면 16, 64, 128 등등

2. 필터의 크기(3, 3)
필터의 크기는 이미지의 어떤 영역을 한 번에 보고 특징을 추출할지를 결정합니다. 3x3 필터는 현재 픽셀과 주변 8개의 픽셀에 대한 정보를 사용하여 특징을 추출합니다.
3x3 크기의 필터는 작지만 효과적인 특징 추출이 가능하다는 것이 많은 연구를 통해 확인되었습니다.
또한, 이 크기는 계산 효율성과 성능 간의 균형을 잘 유지하며, VGGNet과 같은 유명한 모델에서도 주로 사용됩니다.
다른 크기의 필터 (예: 5x5, 7x7)도 사용될 수 있지만, 모델의 복잡성과 연산량에 영향을 줄 수 있습니다.

3. 활성화함수
ReLU(Rectified Linear Unit)는 현재 가장 널리 사용되는 활성화 함수 중 하나입니다. ReLU는 음수 값을 0으로, 양수 값은 그대로 출력하는 간단한 함수입니다.
ReLU는 학습이 빠르며, 다양한 문제에 잘 작동한다는 것이 경험적으로 알려져 있습니다.
기존의 시그모이드나 하이퍼볼릭 탄젠트와 같은 활성화 함수는 그래디언트 소실 문제 (vanishing gradient problem) 때문에 깊은 네트워크에서 학습이 어려울 수 있습니다. ReLU는 이 문제를 어느 정도 해결해 줍니다. 
'''

# 풀링 계층
model.add(layers.Maxpooling2D((2, 2)))
# MaxPooling2D는 최대 풀링 연산을 나타냅니다. (2, 2)는 풀링 크기를 나타내며, 이 경우 2x2 크기의 영역에서 가장 큰 값을 선택하여 다운샘플링합니다.

'''
풀링계층
차원축소 : 풀링을 통해 피처 맵의 크기를 줄이면 계산량이 줄어들고 이는 모델의 효율성 향상을 도와 줍니다.
과적합방지 : 차원을 줄임으로써 파라미터의 수를 줄이게 되고, 이는 과적합의 위험을 감소 시켜 줍니다.
이미지의 미세한 변화에 둔감하게 만듦 : 풀링은 지역적인 작은 변화에는 덜 민감하게 만들어, 이미지의 전반적인 특징을 학습하는 데 도움을 줍니다.

2x2를 쓰는 이유
1. 계산 효율성 : 2x2풀링은 피처 맵의 크기를 절반으로 줄입니다. 이는 계산량을 크게 감소시키면서도 주요한 특징을 유지할 수 있습니다.
2. 보편적인 효과 : 2x2풀링은 다양한 테스크와 데이터셋에서 잘 작동하는 것으로 알려져 있습니다. 이보다 큰 풀링 크기(3x3, 4x4)는 너무 많은 정보를 손실할 수 있으며, 이보다 작은 풀링 크기(1x1)는 차원 축소의 효과가 없습니다.
3. 위치 변화에 대한 둔감성 : 2x2풀링은 이미지 내 작은 위치 변화에도 둔감하게 만들어 줍니다. 이는 객체의 위치가 약간 변경될 때도 모델이 일관된 피처를 추출할 수 있도록 합니다.

그렇다고 2x2 풀링이 항상 최적이라는것은 아님 : 테스크와 데이터에 따라 다른 풀링 크기나 전략을 활용할 수 있으나 많은 경우에서 2x2 풀링은 효과적인 결과를 제공합니다.

예시) 
1. 이미지 준비 : 가령 4x4 픽셀로 구성 된 이미지
[
    [23, 45, 89, 12]
    [45, 22, 12, 45]
    [67, 89, 56, 34]
    [12, 34, 78, 90]
]

2. 2x2 풀링 필터 적용 : 일반적으로 최대 풀링이 많이 사용됨(max pooling) 2x2영역에서 가장 큰 값을 선택
    - 첫 번째 2x2 영역은 [23, 45, 45, 22] 이중 가장 큰 값은 45
    - 두 번째 2x2 영역은 [89, 12, 12, 45] 이중 가장 큰 값은 89
    - 세 번째 2x2 영역은 [67, 89, 12, 34] 이중 가장 큰 값은 89
    - 네 번째 2x2 영역은 [56, 34, 78, 90] 이중 가장 큰 값은 90
    - 이런방식으로 전체 이미지에 풀링을 적용하면 다음과 같은 2x2 크기의 새로운 피처맵을 얻게 됨
    [
        [45, 89]
        [89, 90]
    ]

3. 결과 : 원래 4x4 크기의 이미지가 2x2 크기로 줄어들었습니다. 이렇게 풀링을 통해 이미지의 크기를 줄이면서도 중요한 특징을 유지할 수 있습니다.

'''

# 여러 계층을 더 추가
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))

# 완전 연결 계층
# 평탄화 연산을 수행하여 2D출력을 1D로 변환한 후, 완전 연결 계층을 추가
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))
'''
Dense 레이어는 완전 연결 계층을 나타냅니다.
첫번 째 Dense 레이어에는 64개의 뉴런을 사용하고, relu 활성화 함수를 사용합니다.
두번 째 Dense 레이어에는 10개의 뉴런을 사용하고, softmax 활성화 함수를 사용합니다.
softmax 활성화 함수는 10개의 뉴런의 출력값을 0~1 사이의 값으로 정규화하며, 이 값들의 합은 1이 됩니다. 이는 10개의 뉴런의 출력값을 확률로 해석할 수 있게 해 줍니다.

평탄화를 해주는 이유 : CNN의 합성곱 계층 및 풀링 계층을 거치면 3차원의 피처 맵(높이, 너비, 깊이)이 생성됩니다. 이 3차원의 데이터를 전통적인 완전 연결계층(DENSE)에 넣기 위해서는 1차원 형태로 변환해 주어야함

'''

# 모델 구조 출력
model.summary()